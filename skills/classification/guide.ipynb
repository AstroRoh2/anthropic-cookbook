{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To complete the following guide you will need to install the following packages:\n",
    "- anthropic \n",
    "- voyageai\n",
    "- pandas\n",
    "- matplotlib\n",
    "- sklearn\n",
    "- numpy\n",
    "\n",
    "You will also need:\n",
    "\n",
    "- Anthropic API Key\n",
    "- VoyageAI API Key (Optional)\n",
    "    - Embeddings are pre-computed but you will need API key if you make any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic\n",
    "!pip install voyageai\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VOYAGE_API_KEY'] = \"VOYAGE KEY HERE\"\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"ANTHROPIC KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our environment\n",
    "import anthropic\n",
    "import os\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Claude\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized the field of classification, particularly in areas where traditional machine learning systems have faced challenges. LLMs have demonstrated remarkable success in handling classification problems characterized by complex business rules and scenarios with low-quality or limited training data. Additionally, LLMs have the capability of providing natural language explanations and justifications for their actions, enhancing the interpretability and transparency of the classification process. By leveraging the power of LLMs, we can build classification systems that go beyond the capabilities of traditional machine learning approaches and excel in scenarios where data is scarce or business requirements are intricate.\n",
    "\n",
    "In this guide, we will explore how LLMs can be leveraged to tackle advanced classification tasks. We will cover the following key components and steps:\n",
    "\n",
    "1. **Data Preparation**: We will begin by preparing our training and test data. The training data will be used to build the classification model, while the test data will be utilized to assess its performance. Proper data preparation is crucial to ensure the effectiveness of our classification system.\n",
    "\n",
    "2. **Prompt Engineering**: Prompt engineering plays a vital role in harnessing the power of LLMs for classification. We will design a prompt template that defines the structure and format of the prompts used for classification. The prompt template will incorporate the user query, class definitions, and relevant examples from the vector database. By carefully crafting the prompts, we can guide the LLM to generate accurate and contextually relevant classifications.\n",
    "\n",
    "3. **Implementing Retrieval-Augmented Generation (RAG)**: To enhance the classification process, we will employ a vector database to store and efficiently retrieve embeddings of our training data. The vector database enables similarity searches, allowing us to find the most relevant examples for a given query. By augmenting the LLM with retrieved examples, we can provide additional context and improve the accuracy of the generated classifications.\n",
    "\n",
    "4. **Testing and Evaluation**: Once our classification system is built, we will rigorously test its performance using the transformed test data. We will iterate over the test queries, classify each query using the classification function, and compare the predicted categories with the expected categories. By analyzing the classification results, we can evaluate the effectiveness of our system and identify areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition: Insurance Support Ticket Classifier\n",
    "\n",
    "*Note: The problem definition, data, and labels used in this example were synthetically generated by Claude 3 Opus*\n",
    "\n",
    "In the insurance industry, customer support plays a crucial role in ensuring client satisfaction and retention. Insurance companies receive a high volume of support tickets daily, covering a wide range of topics such as billing, policy administration, claims assistance, and more. Manually categorizing these tickets can be time-consuming and inefficient, leading to longer response times and potentially impacting customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Definitions\n",
    "\n",
    "1. Billing Inquiries\n",
    "- Questions about invoices, charges, fees, and premiums\n",
    "- Requests for clarification on billing statements\n",
    "- Inquiries about payment methods and due dates\n",
    "\n",
    "2. Policy Administration\n",
    "- Requests for policy changes, updates, or cancellations\n",
    "- Questions about policy renewals and reinstatements\n",
    "- Inquiries about adding or removing coverage options\n",
    "\n",
    "3. Claims Assistance\n",
    "- Questions about the claims process and filing procedures\n",
    "- Requests for help with submitting claim documentation\n",
    "- Inquiries about claim status and payout timelines\n",
    "\n",
    "4. Coverage Explanations\n",
    "- Questions about what is covered under specific policy types\n",
    "- Requests for clarification on coverage limits and exclusions\n",
    "- Inquiries about deductibles and out-of-pocket expenses\n",
    "\n",
    "\n",
    "5. Quotes and Proposals\n",
    "- Requests for new policy quotes and price comparisons\n",
    "- Questions about available discounts and bundling options\n",
    "- Inquiries about switching from another insurer\n",
    "\n",
    "\n",
    "6. Account Management\n",
    "- Requests for login credentials or password resets\n",
    "- Questions about online account features and functionality\n",
    "- Inquiries about updating contact or personal information\n",
    "\n",
    "\n",
    "7. Billing Disputes\n",
    "- Complaints about unexpected or incorrect charges\n",
    "- Requests for refunds or premium adjustments\n",
    "- Inquiries about late fees or collection notices\n",
    "\n",
    "\n",
    "8. Claims Disputes\n",
    "- Complaints about denied or underpaid claims\n",
    "- Requests for reconsideration of claim decisions\n",
    "- Inquiries about appealing a claim outcome\n",
    "\n",
    "\n",
    "9. Policy Comparisons\n",
    "- Questions about the differences between policy options\n",
    "- Requests for help deciding between coverage levels\n",
    "- Inquiries about how policies compare to competitors' offerings\n",
    "\n",
    "\n",
    "10. General Inquiries\n",
    "- Questions about company contact information or hours of operation\n",
    "- Requests for general information about products or services\n",
    "- Inquiries that don't fit neatly into other categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeled Data\n",
    "\n",
    "The data can be found in this [Google Sheet](https://docs.google.com/spreadsheets/d/1UwbrWCWsTFGVshyOfY2ywtf5BEt7pUcJEGYZDkfkufU/edit#gid=563251048) you can also find the same data in the `data` folder.\n",
    "\n",
    "We will use the following datasets:\n",
    "- `./data/test.tsv`\n",
    "- `./data/train/tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    'test_2': []\n",
    "}\n",
    "\n",
    "# Helper function to convert a DataFrame to a list of dictionaries\n",
    "def dataframe_to_dict_list(df):\n",
    "    return df.apply(lambda x: {'text': x['text'], 'label': x['label']}, axis=1).tolist()\n",
    "\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "test_df = pd.read_csv(\"./data/test.tsv\", sep='\\t')\n",
    "data['test'] = dataframe_to_dict_list(test_df)\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train.tsv\", sep='\\t')\n",
    "data['train'] = dataframe_to_dict_list(train_df)\n",
    "\n",
    "\n",
    "# Understand the labels in the dataset\n",
    "labels = list(set(train_df['label'].unique()))\n",
    "\n",
    "# Print the first training example and the number of training examples\n",
    "print(data['train'][0], len(data['train']))\n",
    "\n",
    "# Create the test set\n",
    "X_test = [example['text'] for example in data['test']]\n",
    "y_test = [example['label'] for example in data['test']]\n",
    "\n",
    "# Print the length of the test set\n",
    "print(len(X_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating each classification model\n",
    "\n",
    "The `evaluate` function takes the following parameters:\n",
    "- `X`: The input features.\n",
    "- `y`: The true labels.\n",
    "- `classifier`: The classifier function to be evaluated.\n",
    "- `batch_size`: The size of each batch for classification (defaults to the tier's maximum batch size).\n",
    "\n",
    "The `plot_confusion_matrix` function takes the following parameters:\n",
    "- `cm`: The confusion matrix.\n",
    "- `labels`: The labels for the classes.\n",
    "\n",
    "By using this evaluation code, you can assess the performance of your classifier and visualize the confusion matrix to gain insights into the model's predictions.\n",
    "\n",
    "Adjust the `MAXIMUM_CONCURRENT_REQUESTS` to match the rate limits associated with your Anthropic accout, [see here](https://docs.anthropic.com/claude/reference/rate-limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "\n",
    "#you can increase this number to speed up evaluation, but keep in mind that you may need a higher API rate limit\n",
    "#see https://docs.anthropic.com/en/api/rate-limits#rate-limits for more details\n",
    "MAXIMUM_CONCURRENT_REQUESTS = 1\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    # Visualize the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # Set tick labels and positions\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    # Add labels to each cell\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, cm[i, j],\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(X, y, classifier, batch_size=MAXIMUM_CONCURRENT_REQUESTS):\n",
    "    # Initialize lists to store the predicted and true labels\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit the classification tasks to the executor in batches\n",
    "        futures = []\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            batch_futures = [executor.submit(classifier, x) for x in batch_X]\n",
    "            futures.extend(batch_futures)\n",
    "\n",
    "        # Retrieve the results in the original order\n",
    "        for i, future in enumerate(futures):\n",
    "            predicted_label = future.result()\n",
    "            y_pred.append(predicted_label)\n",
    "            y_true.append(y[i])\n",
    "\n",
    "    # Normalize y_true and y_pred\n",
    "    y_true = [label.strip() for label in y_true]\n",
    "    y_pred = [label.strip() for label in y_pred]\n",
    "\n",
    "    # Calculate the classification metrics\n",
    "    report = classification_report(y_true, y_pred, labels=labels, zero_division=1)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print(report)\n",
    "    plot_confusion_matrix(cm, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Classifier\n",
    "\n",
    "To demonstrate the output of our evaluate function we can define a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_classifier(text):\n",
    "    return random.choice(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the random classification method on the test set...\")\n",
    "evaluate(X_test, y_test, random_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classification Test\n",
    "\n",
    "Now lets construct a simple classifier using Claude.\n",
    "\n",
    "First we will encode the categories in XML format. This will make it easier for Claude to interpret the information. Encoding information in XML is a general prompting strategy, for more information [see here](https://docs.anthropic.com/claude/docs/use-xml-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "categories = textwrap.dedent(\"\"\"<category> \n",
    "    <label>Billing Inquiries</label>\n",
    "    <content> Questions about invoices, charges, fees, and premiums Requests for clarification on billing statements Inquiries about payment methods and due dates \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Policy Administration</label>\n",
    "    <content> Requests for policy changes, updates, or cancellations Questions about policy renewals and reinstatements Inquiries about adding or removing coverage options \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Claims Assistance</label> \n",
    "    <content> Questions about the claims process and filing procedures Requests for help with submitting claim documentation Inquiries about claim status and payout timelines \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Coverage Explanations</label> \n",
    "    <content> Questions about what is covered under specific policy types Requests for clarification on coverage limits and exclusions Inquiries about deductibles and out-of-pocket expenses \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Quotes and Proposals</label> \n",
    "    <content> Requests for new policy quotes and price comparisons Questions about available discounts and bundling options Inquiries about switching from another insurer \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Account Management</label> \n",
    "    <content> Requests for login credentials or password resets Questions about online account features and functionality Inquiries about updating contact or personal information \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Billing Disputes</label> \n",
    "    <content> Complaints about unexpected or incorrect charges Requests for refunds or premium adjustments Inquiries about late fees or collection notices \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Claims Disputes</label> \n",
    "    <content> Complaints about denied or underpaid claims Requests for reconsideration of claim decisions Inquiries about appealing a claim outcome \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>Policy Comparisons</label> \n",
    "    <content> Questions about the differences between policy options Requests for help deciding between coverage levels Inquiries about how policies compare to competitors' offerings \n",
    "    </content> \n",
    "</category> \n",
    "<category> \n",
    "    <label>General Inquiries</label> \n",
    "    <content> Questions about company contact information or hours of operation Requests for general information about products or services Inquiries that don't fit neatly into other categories \n",
    "    </content> \n",
    "</category>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will construct a classify function that does the following things:\n",
    "- Defines the prompt template\n",
    "- Inputs the variables in out prompt template\n",
    "- Extracts a normalized response\n",
    "\n",
    "Notice that we leverage the `role: assistant` message and the `stop_sequences` parameter to repeatably extract the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classify(X):\n",
    "    prompt = textwrap.dedent(\"\"\"\n",
    "    You will classify a customer support ticket into one of the following categories:\n",
    "    <categories>\n",
    "        {{categories}}\n",
    "    </categories>\n",
    "\n",
    "    Here is the customer support ticket:\n",
    "    <ticket>\n",
    "        {{ticket}}\n",
    "    </ticket>\n",
    "\n",
    "    Respond with just the label of the category between category tags.\n",
    "    \"\"\").replace(\"{{categories}}\", categories).replace(\"{{ticket}}\", X)\n",
    "    response = client.messages.create( \n",
    "        messages=[{\"role\":\"user\", \"content\": prompt}, {\"role\":\"assistant\", \"content\": \"<category>\"}],\n",
    "        stop_sequences=[\"</category>\"], \n",
    "        max_tokens=4096, \n",
    "        temperature=0.0,\n",
    "        model=\"claude-3-haiku-20240307\"\n",
    "    )\n",
    "    \n",
    "    # Extract the result from the response\n",
    "    result = response.content[0].text.strip()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the simple classification method on the test set...\")\n",
    "evaluate(X_test, y_test, simple_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are better than random but can surely be improved! Lets add RAG with K-shot examples to the prompt.\n",
    "\n",
    "To do this we will need to leverage a VectorDB, this will allow us to match a given query with similar examples from the training data. These examples will hopefully help increase the accuracy of our classifier\n",
    "\n",
    "We will build a simple VectorDB class that leverages the embedding models created by [VoyageAI](https://docs.anthropic.com/en/docs/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import voyageai\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = \"./data/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, data):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts = [item[\"text\"] for item in data]\n",
    "\n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = [item for item in data]\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the vector db and load our training data.\n",
    "\n",
    "VoyageAI has a rate limit of 3RPM for accounts without an associated credit card. For ease of demonstration we will leverage a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = VectorDB()\n",
    "vectordb.load_data(data[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Classification Prompt\n",
    "\n",
    "In this prompt we are leveraging Retrieval Augmented Generation (RAG) to insert examples from the training data that have semantically similar queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_classify(X):\n",
    "    rag = vectordb.search(X,5)\n",
    "    rag_string = \"\"\n",
    "    for example in rag:\n",
    "        rag_string += textwrap.dedent(f\"\"\"\n",
    "        <example>\n",
    "            <query>\n",
    "                \"{example[\"metadata\"][\"text\"]}\"\n",
    "            </query>\n",
    "            <label>\n",
    "                {example[\"metadata\"][\"label\"]}\n",
    "            </label>\n",
    "        </example>\n",
    "        \"\"\")\n",
    "    prompt = textwrap.dedent(\"\"\"\n",
    "    You will classify a customer support ticket into one of the following categories:\n",
    "    <categories>\n",
    "        {{categories}}\n",
    "    </categories>\n",
    "\n",
    "    Here is the customer support ticket:\n",
    "    <ticket>\n",
    "        {{ticket}}\n",
    "    </ticket>\n",
    "\n",
    "    Use the following examples to help you classify the query:\n",
    "    <examples>\n",
    "        {{examples}}\n",
    "    </examples>\n",
    "\n",
    "    Respond with just the label of the category between category tags.\n",
    "    \"\"\").replace(\"{{categories}}\", categories).replace(\"{{ticket}}\", X).replace(\"{{examples}}\", rag_string)\n",
    "    response = client.messages.create( \n",
    "        messages=[{\"role\":\"user\", \"content\": prompt}, {\"role\":\"assistant\", \"content\": \"<category>\"}],\n",
    "        stop_sequences=[\"</category>\"], \n",
    "        max_tokens=4096, \n",
    "        temperature=0.0,\n",
    "        model=\"claude-3-haiku-20240307\"\n",
    "    )\n",
    "    \n",
    "    # Extract the result from the response\n",
    "    result = response.content[0].text.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the RAG method on the test set...\")\n",
    "evaluate(X_test, y_test, rag_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Classification with Chain of Thought Prompt\n",
    "\n",
    "This prompt will build on the previous, by allowing Claude to reflect on the results we can improve the accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain_of_thought_classify(X):\n",
    "    rag = vectordb.search(X,5)\n",
    "    rag_string = \"\"\n",
    "    for example in rag:\n",
    "        rag_string += textwrap.dedent(f\"\"\"\n",
    "        <example>\n",
    "            <query>\n",
    "                \"{example[\"metadata\"][\"text\"]}\"\n",
    "            </query>\n",
    "            <label>\n",
    "                {example[\"metadata\"][\"label\"]}\n",
    "            </label>\n",
    "        </example>\n",
    "        \"\"\")\n",
    "    prompt = textwrap.dedent(\"\"\"\n",
    "    You will classify a customer support ticket into one of the following categories:\n",
    "    <categories>\n",
    "        {{categories}}\n",
    "    </categories>\n",
    "\n",
    "    Here is the customer support ticket:\n",
    "    <ticket>\n",
    "        {{ticket}}\n",
    "    </ticket>\n",
    "\n",
    "    Use the following examples to help you classify the query:\n",
    "    <examples>\n",
    "        {{examples}}\n",
    "    </examples>\n",
    "\n",
    "    First you will think step-by-step about the problem in scratchpad tags.\n",
    "    You should consider all the information provided and create a concrete argument for your classification.\n",
    "    \n",
    "    Respond using this format:\n",
    "    <response>\n",
    "        <scratchpad>Your thoughts and analysis go here</scratchpad>\n",
    "        <category>The category label you chose goes here</category>\n",
    "    </response>\n",
    "    \"\"\").replace(\"{{categories}}\", categories).replace(\"{{ticket}}\", X).replace(\"{{examples}}\", rag_string)\n",
    "    response = client.messages.create( \n",
    "        messages=[{\"role\":\"user\", \"content\": prompt}, {\"role\":\"assistant\", \"content\": \"<response><scratchpad>\"}],\n",
    "        stop_sequences=[\"</category>\"], \n",
    "        max_tokens=4096, \n",
    "        temperature=0.0,\n",
    "        model=\"claude-3-haiku-20240307\"\n",
    "    )\n",
    "    \n",
    "    # Extract the result from the response\n",
    "    result = response.content[0].text.split(\"<category>\")[1].strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the RAG method with Chain of Thought on the test set...\")\n",
    "evaluate(X_test, y_test, rag_chain_of_thought_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This guide has illustrated the importance of measuring prompt performance empirically when prompt engineering. You can read more about our empirical methodology to prompt engineering [here](https://docs.anthropic.com/en/docs/prompt-engineering). Using a Jupyter Notebook is a great way to start prompt engineering but as your datasets grow larger and your prompts more numerous it is important to leverage tooling that will scale with you. \n",
    "\n",
    "In this section of the guide we will explore using [Promptfoo](https://www.promptfoo.dev/) an open source LLM evaluation toolkit. To get started head over to the `./evaluation` directory and checkout the `./evaluation/README.md`.\n",
    "\n",
    "When you have successfully run an evaluation come back here to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "promptfoo_results = pd.read_csv(\"./data/results.csv\")\n",
    "examples_columns = promptfoo_results.columns[2:]\n",
    "\n",
    "number_of_providers = 5\n",
    "number_of_prompts = 3\n",
    "\n",
    "prompts = [\"Simple\", \"RAG\", \"RAG w/ CoT\"]\n",
    "\n",
    "columns = [\"label\", \"text\"] + [\n",
    "    json.loads(examples_columns[prompt * number_of_providers + provider])[\"provider\"]\n",
    "    + \" Prompt: \"\n",
    "    + str(prompts[prompt])\n",
    "    for prompt in range(number_of_prompts)\n",
    "    for provider in range(number_of_providers)\n",
    "]\n",
    "\n",
    "promptfoo_results.columns = columns\n",
    "\n",
    "result = (\n",
    "    promptfoo_results.iloc[:, 2:]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: x.str.count(\"PASS\"))\n",
    "    .sum()\n",
    "    / len(promptfoo_results)\n",
    "    * 100\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
